# ماڈیول 4: ویژن-لینگویج-ایکشن (VLA)

## جائزہ

**ویژن-لینگویج-ایکشن (VLA)** سسٹمز ہیومنائیڈ روبوٹس کو یہ کرنے کے قابل بناتے ہیں:
1. قدرتی زبان کے احکامات (آواز یا متن) **سمجھیں**
2. ویژن (کیمرے، ڈیپتھ سینسرز) کا استعمال کرتے ہوئے ماحول کا **ادراک** کریں
3. بڑے زبان ماڈلز (LLMs) کا استعمال کرتے ہوئے عمل کے سلسلے کی **منصوبہ بندی** کریں
4. ROS 2 کنٹرولرز کے ذریعے اعمال **عملدرآمد** کریں

**ہیومنائیڈ روبوٹکس کے لیے VLA کیوں؟**
- **قدرتی تعامل**: صارفین "باورچی خانے میں جاؤ اور مجھے پانی لاؤ" جیسے احکامات دیتے ہیں
- **کاگنیٹو پلاننگ**: LLMs کثیر مرحلہ کاموں کے بارے میں سوچتے ہیں ("پانی لانے کے لیے، پہلے مجھے نیویگیٹ کرنا ہوگا، پھر گلاس تلاش کرنا ہوگا، پھر اسے پکڑنا ہوگا")
- **ملٹی موڈل ادراک**: ویژن (آبجیکٹ ڈیٹیکشن) کو زبان (صارف کا ارادہ) کے ساتھ ملائیں
- **جنرلائزیشن**: LLMs ٹاسک مخصوص پروگرامنگ کے بغیر نئے احکامات سنبھال سکتے ہیں

## سیکھنے کے نتائج

اس ماڈیول کو مکمل کرنے کے بعد، آپ سمجھیں گے:
1. ✅ Whisper آواز کے احکامات کو متن میں کیسے نقل کرتا ہے
2. ✅ LLMs (GPT-4، Claude) زبان سے عمل کے سلسلے کی منصوبہ بندی کیسے کرتے ہیں
3. ✅ سکل پرائمیٹوز LLM آؤٹ پٹ کو ROS 2 ایکشنز سے کیسے جوڑتے ہیں
4. ✅ ملٹی موڈل ماڈلز (ویژن + زبان) روبوٹ کاگنیشن کو کیسے بڑھاتے ہیں

## اہم تصورات

### 1. وائس ٹو ایکشن پائپ لائن

```mermaid
graph LR
    A[مائیکروفون] -->|آڈیو| B[Whisper]
    B -->|متن: 'باورچی خانے جاؤ'| C[LLM پلانر]
    C -->|سکل: navigate_to('kitchen')| D[ROS 2 ایکشن سرور]
    D -->|/cmd_vel| E[روبوٹ بیس کنٹرولر]
```

**Whisper**: OpenAI کا اسپیچ ٹو ٹیکسٹ ماڈل (کثیر زبانی، شور کے خلاف مضبوط)

### 2. LLM پر مبنی پلاننگ

**ان پٹ**: صارف کا حکم + منظر کی تفصیل
**آؤٹ پٹ**: سکل پرائمیٹوز کا سلسلہ

**مثال**:
```
صارف: "میز سے سرخ کپ لاؤ"
LLM آؤٹ پٹ:
1. navigate_to("table")
2. detect_object("red cup")
3. grasp_object("red cup")
4. navigate_to("user")
5. release_object()
```

**LLMs کیوں؟**
- **فیو شاٹ لرننگ**: مثالیں دیں، LLM نئے احکامات پر عام کرتا ہے
- **عام فہم استدلال**: "کچھ پکڑنے کے لیے، پہلے آپ کو اس تک جانا ہوگا"
- **ایرر ہینڈلنگ**: اگر مرحلہ ناکام ہو، LLM دوبارہ منصوبہ بنا سکتا ہے

### 3. سکل پرائمیٹوز

**تعریف**: فنکشنز کے طور پر ایکسپوز کردہ دوبارہ استعمال کے قابل روبوٹ ایکشنز

**ہیومنائیڈ روبوٹس کے لیے بنیادی سکلز**:
- `navigate_to(location: str)` → نامزد مقام پر جائیں
- `pick_object(target: str)` → نام/تفصیل سے شے پکڑیں
- `scan_environment()` → آبجیکٹ میپ بنانے کے لیے کیمرے استعمال کریں
- `speak(text: str)` → ٹیکسٹ ٹو اسپیچ آؤٹ پٹ

**نفاذ**: ہر سکل ROS 2 ایکشن یا سروس پر میپ کرتا ہے

### 4. ملٹی موڈل تعامل

**ویژن + زبان**:
- **آبجیکٹ گراؤنڈنگ**: "سرخ کپ لاؤ" → LLM ویژن ماڈل استعمال کرتا ہے یہ شناخت کرنے کے لیے کہ کون سی شے "سرخ کپ" ہے
- **مقامی استدلال**: "کتاب بائیں شیلف پر رکھو" → ویژن روبوٹ پوز کے لحاظ سے "بائیں" کا تعین کرتا ہے

**ویژن ماڈلز**:
- **CLIP**: زیرو شاٹ آبجیکٹ کلاسیفیکیشن ("کیا یہ شے کپ ہے؟")
- **OWL-ViT**: اوپن ووکیبلری آبجیکٹ ڈیٹیکشن ("منظر میں تمام کپ تلاش کریں")

## شرائط

- **سافٹ ویئر**: ROS 2 (ماڈیول 1)، آئزک ROS ادراک (ماڈیول 3)، OpenAI/Anthropic API
- **ہارڈویئر**: Jetson Orin NX (100 TOPS) یا کلاؤڈ کمپیوٹ (LLM انفرنس کے لیے)
- **تجربہ**: ایڈوانسڈ (ROS 2، ادراک، اور LLM APIs کی سمجھ ضروری ہے)

### ہارڈویئر کی سفارشات

| جزو | کم از کم | تجویز کردہ |
|-----------|---------|-------------|
| **ایج اے آئی کٹ** | Jetson Orin Nano (40 TOPS) | Jetson Orin NX (100 TOPS) |
| **مائیکروفون** | USB مائک ($10) | شور منسوخی والا ارے مائک ($50) |
| **کیمرا** | RealSense D435i ($200) | Stereolabs ZED 2i ($450) |

**کلاؤڈ متبادل**: اگر Jetson کے پاس محدود کمپیوٹ ہے تو کلاؤڈ (OpenAI API، Anthropic Claude) پر LLM انفرنس چلائیں

## دوسرے ماڈیولز سے تعلق

**← ماڈیول 1 (ROS 2)**: سکلز ROS 2 ایکشنز اور سروسز کے طور پر نافذ ہیں
**← ماڈیول 2 (سمیولیشن)**: ہارڈویئر سے پہلے گزیبو/یونٹی میں VLA پائپ لائنز کی جانچ کریں
**← ماڈیول 3 (آئزک)**: آبجیکٹ ڈیٹیکشن اور نیویگیشن کے لیے آئزک ROS ادراک استعمال کریں
**→ کیپسٹون**: VLA کاگنیٹو لیئر ہے جو تمام دیگر ماڈیولز کو آرکیسٹریٹ کرتا ہے

## اگلے اقدامات

**تکرار 2 میں**، آپ سیکھیں گے:
- ریئل ٹائم وائس ٹرانسکرپشن کے لیے Whisper کیسے انٹیگریٹ کریں
- روبوٹ ٹاسک پلاننگ کے لیے LLMs کو کیسے پرامپٹ کریں
- ROS 2 Python میں سکل پرائمیٹوز کیسے نافذ کریں
- آئزک ROS کے ساتھ Jetson Orin پر VLA سسٹمز کیسے تعینات کریں

## حوالہ جات

Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). *Robust Speech Recognition via Large-Scale Weak Supervision*. arXiv preprint arXiv:2212.04356.

Ahn, M., Brohan, A., Brown, N., et al. (2022). *Do As I Can, Not As I Say: Grounding Language in Robotic Affordances*. Conference on Robot Learning (CoRL).

Driess, D., Xia, F., Sajjadi, M. S., et al. (2023). *PaLM-E: An Embodied Multimodal Language Model*. arXiv preprint arXiv:2303.03378.
